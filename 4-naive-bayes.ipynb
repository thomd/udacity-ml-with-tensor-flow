{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e9a0cf-bbea-43a0-97a4-d34c21fa4799",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "One of the **simplest** yet effective algorithm that should be tried to solve the classification problem is Naive Bayes Algorithm. It’s a probabilistic modell which is based on the Bayes’ theorem which is an equation describing the **relationship of conditional probabilities of statistical quantities**.\n",
    "\n",
    "The Naive Bayes algorithm has **hardly any hyperparameters** and is recommended to use first in the event of classification problems. If this does not give satisfactory results, then more complex algorithms should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc3962-2703-434c-ace7-df70e8afd555",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "The probability of $A$ **given we already know** that $B$ has occured, is defined as\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Only the _portion_ of $A$ that is contained in $B$ could occur, hence the original probability of $A \\cap B$ must be recalculated (or **scaled**) to reflect the fact that the _new_ sample space is $B$.\n",
    "\n",
    "In a slightly redundandt way, the conditional probability can also be written as\n",
    "\n",
    "$$\n",
    "P(A|B) = P(A \\cap B\\,|B)\n",
    "$$\n",
    "\n",
    "which makes it easier to see how we calculate the probability for an event $A$: Writing $P(A \\cap B\\,|B)$ makes it obvious that we want the probability that an event $A \\cap B$ happens, **scaled by the knowledge we already have** about the event $B$:\n",
    "\n",
    "$$\n",
    "P({\\color{orange}{A \\cap B}}\\,|{\\color{purple}B}) = \\frac{P({\\color{orange}{A \\cap B}})}{P({\\color{purple}B})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b63f1-18b5-424e-bdc2-6cb021576a72",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Bayes theorem describes the probability of an event, based on **prior knowledge** that might be related to the event. For example, if the risk of health problems is known to increase with age, Bayes theorem allows the risk to an individual of a known age to be assessed more accurately than simply assuming that the individual is typical of the population as a whole.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(A|B) &= \\frac{P(B|A) \\cdot P(A)}{P(B)} \\\\[10pt]\n",
    "\\text{Posterior} &= \\, \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Evidence}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "* the conditional probability $P(A|B)$ of event $A$ occurring given that $B$ is true. This is also called **posterior probability**.\n",
    "* the conditional probability $P(B|A)$ of event $B$ occurring given that $A$ is true. This is also called the **likelyhood**.\n",
    "* the probability $P(A)$. This is also called the **prior probability**.\n",
    "* the probability $P(B)$. This is also called the **evidence** which **normalizes** our probabilities.\n",
    "\n",
    "See also [Bayes' Theorem with Lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego).\n",
    "\n",
    "### Alternative Form\n",
    "\n",
    "Another form of Bayes theorem for **two competing statements** or hypotheses is\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A)}\n",
    "$$\n",
    "\n",
    "For proposition $A$ and evidence or background $B$,\n",
    "\n",
    "* $P(A)$ is the prior probability, the initial degree of belief in $A$.\n",
    "* $P(\\neg A)$ is the corresponding initial degree of belief in not $A$, that $A$ is false, where $P(\\neg A) = 1 - P(A)$\n",
    "* $P(B|A)$ is the conditional probability or likelihood, the degree of belief in $B$ given that proposition $A$ is true.\n",
    "* $P(B|\\neg A)$ is the conditional probability or likelihood, the degree of belief in $B$ given that proposition $A$ is false.\n",
    "* $P(A|B)$ is the posterior probability, the probability of $A$ after taking into account $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77a7f5-f260-4efe-8ae6-3278a645b947",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Given a medical test having a **99% accuracy** (for true positives and true negatives). Already knowing that **1 out of 10000 people are sick**, what is the probability of being sick?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "![false-positives](images/bayes-theorem.svg)\n",
    "\n",
    "What we knew before we knew the test is positive, is the prior probability $P(sick) = 0.0001$ and $P(healthy) = 0.9999$. As only the **positive tests** actually occured, we scale the likelyhood and the prior with them.\n",
    "\n",
    "The **posterior probability**, what we infered after we knew that the test is positive, is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(sick|positive) &= \\frac{P(sick) \\cdot P(positive|sick)}{P(sick) \\cdot P(positive|sick) + P(healthy) \\cdot P(positive|healthy)} \\\\[10pt]\n",
    "&= \\frac{0.0001 \\cdot 0.99}{0.0001 \\cdot 0.99 + 0.9999 \\cdot 0.01} \\\\[10pt]\n",
    "&= 0.0098 \\approx 1 \\%\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d4935-abde-4d10-922d-c95ec8a08f32",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a612a-219a-4886-9b57-c4114ee114e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

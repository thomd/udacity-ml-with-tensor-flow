{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e9a0cf-bbea-43a0-97a4-d34c21fa4799",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "One of the **simplest** yet effective algorithm that should be tried to solve the classification problem is Naive Bayes Algorithm. It’s a probabilistic modell which is based on the Bayes’ theorem which is an equation describing the **relationship of conditional probabilities of statistical quantities**.\n",
    "\n",
    "The Naive Bayes algorithm has **hardly any hyperparameters** and is recommended to use first in the event of classification problems. If this does not give satisfactory results, then more complex algorithms should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc3962-2703-434c-ace7-df70e8afd555",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "The probability of $A$ **given we already know** that $B$ has occured, is defined as\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Only the _portion_ of $A$ that is contained in $B$ could occur, hence the original probability of $A \\cap B$ must be recalculated (or **scaled**) to reflect the fact that the _new_ sample space is $B$.\n",
    "\n",
    "In a slightly redundandt way, the conditional probability can also be written as\n",
    "\n",
    "$$\n",
    "P(A|B) = P(A \\cap B\\,|B)\n",
    "$$\n",
    "\n",
    "which makes it easier to see how we calculate the probability for an event $A$: Writing $P(A \\cap B\\,|B)$ makes it obvious that we want the probability that an event $A \\cap B$ happens, **scaled by the knowledge we already have** about the event $B$:\n",
    "\n",
    "$$\n",
    "P({\\color{orange}{A \\cap B}}\\,|{\\color{purple}B}) = \\frac{P({\\color{orange}{A \\cap B}})}{P({\\color{purple}B})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b63f1-18b5-424e-bdc2-6cb021576a72",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "See also [Bayes' Theorem with Lego](https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego).\n",
    "\n",
    "Bayes theorem describes the probability of an event, based on **prior knowledge** that might be related to the event. For example, if the risk of health problems is known to increase with age, Bayes theorem allows the risk to an individual of a known age to be assessed more accurately than simply assuming that the individual is typical of the population as a whole.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(A|B) &= \\frac{P(B|A) \\cdot P(A)}{P(B)} \\\\[10pt]\n",
    "\\text{Posterior} &= \\, \\frac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Evidence}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "* the conditional probability $P(A|B)$ of event $A$ occurring given that $B$ is true. This is also called **posterior probability**.\n",
    "* the conditional probability $P(B|A)$ of event $B$ occurring given that $A$ is true. This is also called the **likelyhood**.\n",
    "* the probability $P(A)$. This is also called the **prior probability**.\n",
    "* the probability $P(B)$. This is also called the **evidence** which **normalizes** our probabilities.\n",
    "\n",
    "If we are only interested in **proportions** of conditional probabilities, we can also write\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(A|B) &\\propto P(B|A) \\cdot P(A) \\\\[10pt]\n",
    "\\text{Posterior} &\\propto \\, \\text{Likelihood} \\cdot \\text{Prior}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Alternative Form\n",
    "\n",
    "Another form of Bayes theorem for **two competing statements** or hypotheses is\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A)}\n",
    "$$\n",
    "\n",
    "For proposition $A$ and evidence or background $B$,\n",
    "\n",
    "* $P(A)$ is the prior probability, the initial degree of belief in $A$.\n",
    "* $P(\\neg A)$ is the corresponding initial degree of belief in not $A$, that $A$ is false, where $P(\\neg A) = 1 - P(A)$\n",
    "* $P(B|A)$ is the conditional probability or likelihood, the degree of belief in $B$ given that proposition $A$ is true.\n",
    "* $P(B|\\neg A)$ is the conditional probability or likelihood, the degree of belief in $B$ given that proposition $A$ is false.\n",
    "* $P(A|B)$ is the posterior probability, the probability of $A$ after taking into account $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77a7f5-f260-4efe-8ae6-3278a645b947",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Given a medical test having a **99% accuracy** (for true positives and true negatives). Already knowing that **1 out of 10000 people are sick**, what is the probability of being sick?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "![false-positives](images/bayes-theorem.svg)\n",
    "\n",
    "What we knew before we knew the test is positive, is the **prior probability** $P(sick) = 0.0001$ and $P(healthy) = 0.9999$. \n",
    "\n",
    "As only the **positive tests** actually occured, we scale the likelyhood and the prior with the **evidence** $P(positive)$ which is the sum $P(sick) \\cdot P(positive|sick) + P(healthy) \\cdot P(positive|healthy)$.\n",
    "\n",
    "The **posterior probability**, what we infered after we knew that the test is positive, is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(sick|positive) &= \\frac{P(sick) \\cdot P(positive|sick)}{P(positive)} \\\\[10pt]\n",
    "&= \\frac{P(sick) \\cdot P(positive|sick)}{P(sick) \\cdot P(positive|sick) + P(healthy) \\cdot P(positive|healthy)} \\\\[10pt]\n",
    "&= \\frac{0.0001 \\cdot 0.99}{0.0001 \\cdot 0.99 + 0.9999 \\cdot 0.01} \\\\[10pt]\n",
    "&= 0.0098 \\approx 1 \\%\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ff1e2-66dd-42f0-956b-638fd7c3649e",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Lets consider a spam classifier with event $A$ **being spam** and event $B$ **containing the words** $w_1, \\dots, w_n$, Naive Bayes considers only proportions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A|B) &\\propto P(B|A) \\cdot P(A) \\\\[6pt]\n",
    "P(spam \\, | \\, w_1, \\dots, w_n) &\\propto P(w_1, \\dots, w_n \\, | \\, spam) \\cdot P(spam)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Naive Bayes does now mathematically wrongly calculate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1, \\dots, w_n \\, | \\, spam) &= P(w_1 \\cap w_2 \\cap \\ldots \\cap w_n \\, | \\, spam) \\\\\n",
    "&= P(w_1|spam) \\cdot P(w_2|spam) \\cdot \\ldots \\cdot P(w_n|spam)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The probability that a word $w_i$ occurs in a spam message is the number $n_i$ of the words $w_i$ in all spam messages related to the total number of words $N$ of all spam messages. We add a value $\\alpha$ to avoid having a probability of $0$ for a word:\n",
    "\n",
    "$$\n",
    "P(w_i|spam) = \\frac{n_i}{N} + \\alpha\n",
    "$$\n",
    "\n",
    "Typically, we set $\\alpha = 1$.\n",
    "\n",
    "Naive Bayes is considered **naive**, because it treats all words of a language as a **bag of words** regardless of the order or context of the words. Ignoring relationships among words has a **high bias**, but because it works well in practice, it has a **low variance**.\n",
    "\n",
    "So, the **naive** bit of the theorem is when it considers each **feature** to be **independent** of each other which may not always be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766adec6-21f7-4fc1-8fb9-365c55c993e0",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "The initial guess that we observe a spam message $P(spam)$ is called a **prior probability**. This guess can be any probability we want, but a common guess is estimated from the training data.\n",
    "\n",
    "Now we multiply the initial guess with the probabilities that the words $w_i$ occur in a normal message: $P(spam) \\cdot P(w_i|spam) \\cdot \\ldots \\cdot P(w_j|spam)$. These probabilities are derived from test-data.\n",
    "\n",
    "This is proportional to the **probability that a message is spam** given the words $w_1, \\dots, w_n$, hence can be considered as a **score**.\n",
    "\n",
    "If we do the same for non-spam messages, we get two scores which tell us whether a message is spam or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6ae4a-e031-4b72-9816-561a85e39c9d",
   "metadata": {},
   "source": [
    "## Naive Bayes in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2cc25a-c0ac-4a87-b8d4-ca2d90c6343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB           # Gaussian Naive Bayes:    continuous value and are not discrete\n",
    "from sklearn.naive_bayes import BernoulliNB          # Bernoulli Naive Bayes:   Multinomial Naive Bayes for boolean classes (e.g. spam/ham)\n",
    "from sklearn.naive_bayes import MultinomialNB        # Multinomial Naive Bayes: probability of observing counts among a number of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529d037-bc2e-4bfd-a017-964d15a9cc71",
   "metadata": {},
   "source": [
    "### Naive Bayes Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6630f3-7b80-48f5-b294-9e358a8218d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
